{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = pd.read_csv('train.csv')\n",
    "testx = pd.read_csv('test.csv')\n",
    "trainy = trainx['Survived']\n",
    "testy1 = pd.read_csv('gender_submission.csv')\n",
    "trainx.drop(['Name', 'Ticket', 'Cabin'], axis = 1, inplace=True)\n",
    "testy = testy1['Survived']\n",
    "testx = testx.fillna(testx.Fare.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    1.000000\n",
       "Pclass     -0.338481\n",
       "Sex        -0.543351\n",
       "Age        -0.017315\n",
       "SibSp      -0.035322\n",
       "Parch       0.081629\n",
       "Fare        0.257307\n",
       "Embarked   -0.176509\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Really useful//\n",
    "trainx.Sex = trainx['Sex'].astype(\"category\").cat.codes\n",
    "testx.Sex = testx['Sex'].astype(\"category\").cat.codes\n",
    "trainx.Embarked = trainx['Embarked'].astype(\"category\").cat.codes\n",
    "# now we will find the coorrelation\n",
    "train= trainx.fillna({'Age': 12, 'Embarked': '1'})\n",
    "train[train.columns[1:]].corr()['Survived'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived    1.000000\n",
       "Pclass     -0.338481\n",
       "Sex        -0.543351\n",
       "Age        -0.077221\n",
       "SibSp      -0.035322\n",
       "Parch       0.081629\n",
       "Fare        0.257307\n",
       "Embarked   -0.176509\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx[trainx.columns[1:]].corr()['Survived'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = trainx.drop(['Survived', 'Age', 'Embarked'], axis=1)\n",
    "testx = testx.drop(['Name', 'Ticket', 'Cabin', 'Age', 'Embarked' ],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = np.asarray(trainx)\n",
    "trainy = np.asarray(trainy)\n",
    "testx = np.asarray(testx)\n",
    "testy = np.asarray(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we normalize the dataset:\n",
    "from sklearn import preprocessing\n",
    "trainx = preprocessing.StandardScaler().fit(trainx).transform(trainx)\n",
    "testx = preprocessing.StandardScaler().fit(testx).transform(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 6)\n",
      "(418,)\n"
     ]
    }
   ],
   "source": [
    "print(testx.shape)\n",
    "print(testy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This function implements logistic regression and can use different numerical optimizers to find parameters, including ‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’ solvers. You can find extensive information about the pros and cons of these optimizers if you search it in internet.\n",
    "\n",
    "The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. Now lets fit our model with train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(trainx,trainy)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = LR.predict(testx)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_proba returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77962854, 0.22037146],\n",
       "       [0.46447538, 0.53552462],\n",
       "       [0.69317934, 0.30682066],\n",
       "       [0.77915323, 0.22084677],\n",
       "       [0.45945337, 0.54054663],\n",
       "       [0.77883201, 0.22116799],\n",
       "       [0.43161755, 0.56838245],\n",
       "       [0.70672884, 0.29327116],\n",
       "       [0.43193525, 0.56806475],\n",
       "       [0.81327488, 0.18672512],\n",
       "       [0.77957496, 0.22042504],\n",
       "       [0.57908477, 0.42091523],\n",
       "       [0.21851912, 0.78148088],\n",
       "       [0.70933551, 0.29066449],\n",
       "       [0.23057377, 0.76942623],\n",
       "       [0.34234332, 0.65765668],\n",
       "       [0.69128887, 0.30871113],\n",
       "       [0.77994206, 0.22005794],\n",
       "       [0.46368054, 0.53631946],\n",
       "       [0.4319135 , 0.5680865 ],\n",
       "       [0.58420442, 0.41579558],\n",
       "       [0.78173243, 0.21826757],\n",
       "       [0.22459154, 0.77540846],\n",
       "       [0.54981231, 0.45018769],\n",
       "       [0.13304312, 0.86695688],\n",
       "       [0.79773019, 0.20226981],\n",
       "       [0.20727264, 0.79272736],\n",
       "       [0.77992607, 0.22007393],\n",
       "       [0.57544005, 0.42455995],\n",
       "       [0.81447574, 0.18552426],\n",
       "       [0.70930287, 0.29069713],\n",
       "       [0.7319883 , 0.2680117 ],\n",
       "       [0.4519406 , 0.5480594 ],\n",
       "       [0.44960092, 0.55039908],\n",
       "       [0.58548834, 0.41451166],\n",
       "       [0.77991092, 0.22008908],\n",
       "       [0.43121006, 0.56878994],\n",
       "       [0.43071452, 0.56928548],\n",
       "       [0.77862345, 0.22137655],\n",
       "       [0.75088295, 0.24911705],\n",
       "       [0.77590641, 0.22409359],\n",
       "       [0.57857635, 0.42142365],\n",
       "       [0.77954959, 0.22045041],\n",
       "       [0.32396866, 0.67603134],\n",
       "       [0.23558405, 0.76441595],\n",
       "       [0.77950246, 0.22049754],\n",
       "       [0.5760408 , 0.4239592 ],\n",
       "       [0.77959802, 0.22040198],\n",
       "       [0.20007349, 0.79992651],\n",
       "       [0.42349485, 0.57650515],\n",
       "       [0.58365785, 0.41634215],\n",
       "       [0.68933545, 0.31066455],\n",
       "       [0.37525879, 0.62474121],\n",
       "       [0.166281  , 0.833719  ],\n",
       "       [0.68894544, 0.31105456],\n",
       "       [0.84728443, 0.15271557],\n",
       "       [0.77950133, 0.22049867],\n",
       "       [0.77963844, 0.22036156],\n",
       "       [0.79683133, 0.20316867],\n",
       "       [0.11951789, 0.88048211],\n",
       "       [0.77949493, 0.22050507],\n",
       "       [0.69039279, 0.30960721],\n",
       "       [0.77957401, 0.22042599],\n",
       "       [0.43141015, 0.56858985],\n",
       "       [0.44974219, 0.55025781],\n",
       "       [0.31819741, 0.68180259],\n",
       "       [0.4312791 , 0.5687209 ],\n",
       "       [0.56577682, 0.43422318],\n",
       "       [0.57692281, 0.42307719],\n",
       "       [0.13243467, 0.86756533],\n",
       "       [0.43137403, 0.56862597],\n",
       "       [0.77947732, 0.22052268],\n",
       "       [0.4312285 , 0.5687715 ],\n",
       "       [0.57756592, 0.42243408],\n",
       "       [0.13822894, 0.86177106],\n",
       "       [0.42782536, 0.57217464],\n",
       "       [0.77938226, 0.22061774],\n",
       "       [0.27720597, 0.72279403],\n",
       "       [0.69070982, 0.30929018],\n",
       "       [0.43135347, 0.56864653],\n",
       "       [0.79679671, 0.20320329],\n",
       "       [0.45171677, 0.54828323],\n",
       "       [0.57892359, 0.42107641],\n",
       "       [0.77945811, 0.22054189],\n",
       "       [0.69230329, 0.30769671],\n",
       "       [0.79766429, 0.20233571],\n",
       "       [0.43123341, 0.56876659],\n",
       "       [0.43109357, 0.56890643],\n",
       "       [0.43133291, 0.56866709],\n",
       "       [0.71063884, 0.28936116],\n",
       "       [0.45863834, 0.54136166],\n",
       "       [0.77951349, 0.22048651],\n",
       "       [0.23481864, 0.76518136],\n",
       "       [0.77935504, 0.22064496],\n",
       "       [0.57889634, 0.42110366],\n",
       "       [0.77949535, 0.22050465],\n",
       "       [0.22030677, 0.77969323],\n",
       "       [0.77941921, 0.22058079],\n",
       "       [0.43122614, 0.56877386],\n",
       "       [0.77934543, 0.22065457],\n",
       "       [0.23378732, 0.76621268],\n",
       "       [0.70916651, 0.29083349],\n",
       "       [0.77950999, 0.22049001],\n",
       "       [0.77949428, 0.22050572],\n",
       "       [0.46229867, 0.53770133],\n",
       "       [0.771053  , 0.228947  ],\n",
       "       [0.77946363, 0.22053637],\n",
       "       [0.77950199, 0.22049801],\n",
       "       [0.77895665, 0.22104335],\n",
       "       [0.69064814, 0.30935186],\n",
       "       [0.68920898, 0.31079102],\n",
       "       [0.43125685, 0.56874315],\n",
       "       [0.22444795, 0.77555205],\n",
       "       [0.43165174, 0.56834826],\n",
       "       [0.15015288, 0.84984712],\n",
       "       [0.79761919, 0.20238081],\n",
       "       [0.78022753, 0.21977247],\n",
       "       [0.45559586, 0.54440414],\n",
       "       [0.53900049, 0.46099951],\n",
       "       [0.34339828, 0.65660172],\n",
       "       [0.32183747, 0.67816253],\n",
       "       [0.80114081, 0.19885919],\n",
       "       [0.23239569, 0.76760431],\n",
       "       [0.77975848, 0.22024152],\n",
       "       [0.77947478, 0.22052522],\n",
       "       [0.42384876, 0.57615124],\n",
       "       [0.77944572, 0.22055428],\n",
       "       [0.4834884 , 0.5165116 ],\n",
       "       [0.69061034, 0.30938966],\n",
       "       [0.77929738, 0.22070262],\n",
       "       [0.77929578, 0.22070422],\n",
       "       [0.57680965, 0.42319035],\n",
       "       [0.41428984, 0.58571016],\n",
       "       [0.80180881, 0.19819119],\n",
       "       [0.77937645, 0.22062355],\n",
       "       [0.77939834, 0.22060166],\n",
       "       [0.77975179, 0.22024821],\n",
       "       [0.69059243, 0.30940757],\n",
       "       [0.43097708, 0.56902292],\n",
       "       [0.77692337, 0.22307663],\n",
       "       [0.56043526, 0.43956474],\n",
       "       [0.16330545, 0.83669455],\n",
       "       [0.41676775, 0.58323225],\n",
       "       [0.68138516, 0.31861484],\n",
       "       [0.57834245, 0.42165755],\n",
       "       [0.83495561, 0.16504439],\n",
       "       [0.55794648, 0.44205352],\n",
       "       [0.77926855, 0.22073145],\n",
       "       [0.57833336, 0.42166664],\n",
       "       [0.70849348, 0.29150652],\n",
       "       [0.19589552, 0.80410448],\n",
       "       [0.77934923, 0.22065077],\n",
       "       [0.76329022, 0.23670978],\n",
       "       [0.42624053, 0.57375947],\n",
       "       [0.84583407, 0.15416593],\n",
       "       [0.77953804, 0.22046196],\n",
       "       [0.13416811, 0.86583189],\n",
       "       [0.43109137, 0.56890863],\n",
       "       [0.57831065, 0.42168935],\n",
       "       [0.42494909, 0.57505091],\n",
       "       [0.43118189, 0.56881811],\n",
       "       [0.79667455, 0.20332545],\n",
       "       [0.32336663, 0.67663337],\n",
       "       [0.77983548, 0.22016452],\n",
       "       [0.6905387 , 0.3094613 ],\n",
       "       [0.45115152, 0.54884848],\n",
       "       [0.5590072 , 0.4409928 ],\n",
       "       [0.80701893, 0.19298107],\n",
       "       [0.22662733, 0.77337267],\n",
       "       [0.43017162, 0.56982838],\n",
       "       [0.77951403, 0.22048597],\n",
       "       [0.7796958 , 0.2203042 ],\n",
       "       [0.7978272 , 0.2021728 ],\n",
       "       [0.77969023, 0.22030977],\n",
       "       [0.78604609, 0.21395391],\n",
       "       [0.3041042 , 0.6958958 ],\n",
       "       [0.30686385, 0.69313615],\n",
       "       [0.58700393, 0.41299607],\n",
       "       [0.30350312, 0.69649688],\n",
       "       [0.19541018, 0.80458982],\n",
       "       [0.69050685, 0.30949315],\n",
       "       [0.56407712, 0.43592288],\n",
       "       [0.23503028, 0.76496972],\n",
       "       [0.77938031, 0.22061969],\n",
       "       [0.13926604, 0.86073396],\n",
       "       [0.68838759, 0.31161241],\n",
       "       [0.31734306, 0.68265694],\n",
       "       [0.82092742, 0.17907258],\n",
       "       [0.63657138, 0.36342862],\n",
       "       [0.69048894, 0.30951106],\n",
       "       [0.70899553, 0.29100447],\n",
       "       [0.57867611, 0.42132389],\n",
       "       [0.79702427, 0.20297573],\n",
       "       [0.69093697, 0.30906303],\n",
       "       [0.67539682, 0.32460318],\n",
       "       [0.77930225, 0.22069775],\n",
       "       [0.48884459, 0.51115541],\n",
       "       [0.4310638 , 0.5689362 ],\n",
       "       [0.69222283, 0.30777717],\n",
       "       [0.43078744, 0.56921256],\n",
       "       [0.42484682, 0.57515318],\n",
       "       [0.77459224, 0.22540776],\n",
       "       [0.44676954, 0.55323046],\n",
       "       [0.34258787, 0.65741213],\n",
       "       [0.69221093, 0.30778907],\n",
       "       [0.57885111, 0.42114889],\n",
       "       [0.43106337, 0.56893663],\n",
       "       [0.69220498, 0.30779502],\n",
       "       [0.22656205, 0.77343795],\n",
       "       [0.77925633, 0.22074367],\n",
       "       [0.77088033, 0.22911967],\n",
       "       [0.77973052, 0.22026948],\n",
       "       [0.64646126, 0.35353874],\n",
       "       [0.34320092, 0.65679908],\n",
       "       [0.55969291, 0.44030709],\n",
       "       [0.56535748, 0.43464252],\n",
       "       [0.43093649, 0.56906351],\n",
       "       [0.49726848, 0.50273152],\n",
       "       [0.15401784, 0.84598216],\n",
       "       [0.77915319, 0.22084681],\n",
       "       [0.35222893, 0.64777107],\n",
       "       [0.77914998, 0.22085002],\n",
       "       [0.3254032 , 0.6745968 ],\n",
       "       [0.77929038, 0.22070962],\n",
       "       [0.22669421, 0.77330579],\n",
       "       [0.42361917, 0.57638083],\n",
       "       [0.77928558, 0.22071442],\n",
       "       [0.43101541, 0.56898459],\n",
       "       [0.77512917, 0.22487083],\n",
       "       [0.69040932, 0.30959068],\n",
       "       [0.65277627, 0.34722373],\n",
       "       [0.22719886, 0.77280114],\n",
       "       [0.80163181, 0.19836819],\n",
       "       [0.77922725, 0.22077275],\n",
       "       [0.57420818, 0.42579182],\n",
       "       [0.77923817, 0.22076183],\n",
       "       [0.57101582, 0.42898418],\n",
       "       [0.7795902 , 0.2204098 ],\n",
       "       [0.35219172, 0.64780828],\n",
       "       [0.20492979, 0.79507021],\n",
       "       [0.22650983, 0.77349017],\n",
       "       [0.31026307, 0.68973693],\n",
       "       [0.52212556, 0.47787444],\n",
       "       [0.77920654, 0.22079346],\n",
       "       [0.79168466, 0.20831534],\n",
       "       [0.58969717, 0.41030283],\n",
       "       [0.31783182, 0.68216818],\n",
       "       [0.70387537, 0.29612463],\n",
       "       [0.34312745, 0.65687255],\n",
       "       [0.45781893, 0.54218107],\n",
       "       [0.34056412, 0.65943588],\n",
       "       [0.77917254, 0.22082746],\n",
       "       [0.52093685, 0.47906315],\n",
       "       [0.77837727, 0.22162273],\n",
       "       [0.77827652, 0.22172348],\n",
       "       [0.77937794, 0.22062206],\n",
       "       [0.77926339, 0.22073661],\n",
       "       [0.77909228, 0.22090772],\n",
       "       [0.32353028, 0.67646972],\n",
       "       [0.77924446, 0.22075554],\n",
       "       [0.79583384, 0.20416616],\n",
       "       [0.77919651, 0.22080349],\n",
       "       [0.31516137, 0.68483863],\n",
       "       [0.45893934, 0.54106066],\n",
       "       [0.69052389, 0.30947611],\n",
       "       [0.7791666 , 0.2208334 ],\n",
       "       [0.59917341, 0.40082659],\n",
       "       [0.77935873, 0.22064127],\n",
       "       [0.4306802 , 0.5693198 ],\n",
       "       [0.77872666, 0.22127334],\n",
       "       [0.53864873, 0.46135127],\n",
       "       [0.77923936, 0.22076064],\n",
       "       [0.18911846, 0.81088154],\n",
       "       [0.45691114, 0.54308886],\n",
       "       [0.77953099, 0.22046901],\n",
       "       [0.34307077, 0.65692923],\n",
       "       [0.69206805, 0.30793195],\n",
       "       [0.70882833, 0.29117167],\n",
       "       [0.71220353, 0.28779647],\n",
       "       [0.69206209, 0.30793791],\n",
       "       [0.4301597 , 0.5698403 ],\n",
       "       [0.79727522, 0.20272478],\n",
       "       [0.4308898 , 0.5691102 ],\n",
       "       [0.45639695, 0.54360305],\n",
       "       [0.45235116, 0.54764884],\n",
       "       [0.77949927, 0.22050073],\n",
       "       [0.77949767, 0.22050233],\n",
       "       [0.5652459 , 0.4347541 ],\n",
       "       [0.77950621, 0.22049379],\n",
       "       [0.77904099, 0.22095901],\n",
       "       [0.56752468, 0.43247532],\n",
       "       [0.43151357, 0.56848643],\n",
       "       [0.77949981, 0.22050019],\n",
       "       [0.56487019, 0.43512981],\n",
       "       [0.77821223, 0.22178777],\n",
       "       [0.77911852, 0.22088148],\n",
       "       [0.33034558, 0.66965442],\n",
       "       [0.8140983 , 0.1859017 ],\n",
       "       [0.56274495, 0.43725505],\n",
       "       [0.77913562, 0.22086438],\n",
       "       [0.77917877, 0.22082123],\n",
       "       [0.68882783, 0.31117217],\n",
       "       [0.68401155, 0.31598845],\n",
       "       [0.7786721 , 0.2213279 ],\n",
       "       [0.43083955, 0.56916045],\n",
       "       [0.25032201, 0.74967799],\n",
       "       [0.50728968, 0.49271032],\n",
       "       [0.77779038, 0.22220962],\n",
       "       [0.55541701, 0.44458299],\n",
       "       [0.45796218, 0.54203782],\n",
       "       [0.77866086, 0.22133914],\n",
       "       [0.77947176, 0.22052824],\n",
       "       [0.77927253, 0.22072747],\n",
       "       [0.430819  , 0.569181  ],\n",
       "       [0.17034557, 0.82965443],\n",
       "       [0.43082788, 0.56917212],\n",
       "       [0.51279985, 0.48720015],\n",
       "       [0.69198667, 0.30801333],\n",
       "       [0.77910517, 0.22089483],\n",
       "       [0.73146178, 0.26853822],\n",
       "       [0.77914672, 0.22085328],\n",
       "       [0.77945338, 0.22054662],\n",
       "       [0.69022416, 0.30977584],\n",
       "       [0.57793586, 0.42206414],\n",
       "       [0.13801529, 0.86198471],\n",
       "       [0.77954812, 0.22045188],\n",
       "       [0.36243417, 0.63756583],\n",
       "       [0.53528492, 0.46471508],\n",
       "       [0.70873029, 0.29126971],\n",
       "       [0.69021022, 0.30978978],\n",
       "       [0.3053638 , 0.6946362 ],\n",
       "       [0.57539246, 0.42460754],\n",
       "       [0.77943814, 0.22056186],\n",
       "       [0.45587745, 0.54412255],\n",
       "       [0.77905601, 0.22094399],\n",
       "       [0.5783491 , 0.4216509 ],\n",
       "       [0.69019628, 0.30980372],\n",
       "       [0.77845869, 0.22154131],\n",
       "       [0.67705437, 0.32294563],\n",
       "       [0.77942456, 0.22057544],\n",
       "       [0.69194101, 0.30805899],\n",
       "       [0.7792237 , 0.2207763 ],\n",
       "       [0.7618983 , 0.2381017 ],\n",
       "       [0.0560796 , 0.9439204 ],\n",
       "       [0.79679516, 0.20320484],\n",
       "       [0.43082645, 0.56917355],\n",
       "       [0.69017637, 0.30982363],\n",
       "       [0.43116076, 0.56883924],\n",
       "       [0.68982118, 0.31017882],\n",
       "       [0.31762387, 0.68237613],\n",
       "       [0.20603546, 0.79396454],\n",
       "       [0.69191918, 0.30808082],\n",
       "       [0.64616321, 0.35383679],\n",
       "       [0.68033141, 0.31966859],\n",
       "       [0.45119793, 0.54880207],\n",
       "       [0.57830368, 0.42169632],\n",
       "       [0.26006777, 0.73993223],\n",
       "       [0.77902852, 0.22097148],\n",
       "       [0.77909995, 0.22090005],\n",
       "       [0.45667166, 0.54332834],\n",
       "       [0.89073495, 0.10926505],\n",
       "       [0.33416871, 0.66583129],\n",
       "       [0.31759763, 0.68240237],\n",
       "       [0.77857578, 0.22142422],\n",
       "       [0.23334719, 0.76665281],\n",
       "       [0.40698159, 0.59301841],\n",
       "       [0.79723942, 0.20276058],\n",
       "       [0.40518886, 0.59481114],\n",
       "       [0.20814229, 0.79185771],\n",
       "       [0.6895275 , 0.3104725 ],\n",
       "       [0.71838197, 0.28161803],\n",
       "       [0.17079698, 0.82920302],\n",
       "       [0.5989363 , 0.4010637 ],\n",
       "       [0.6901226 , 0.3098774 ],\n",
       "       [0.21769302, 0.78230698],\n",
       "       [0.11920854, 0.88079146],\n",
       "       [0.49487855, 0.50512145],\n",
       "       [0.69116705, 0.30883295],\n",
       "       [0.55892185, 0.44107815],\n",
       "       [0.84556062, 0.15443938],\n",
       "       [0.77906468, 0.22093532],\n",
       "       [0.77899004, 0.22100996],\n",
       "       [0.4252351 , 0.5747649 ],\n",
       "       [0.45616811, 0.54383189],\n",
       "       [0.69018847, 0.30981153],\n",
       "       [0.31338069, 0.68661931],\n",
       "       [0.77904093, 0.22095907],\n",
       "       [0.69009472, 0.30990528],\n",
       "       [0.77905186, 0.22094814],\n",
       "       [0.83285251, 0.16714749],\n",
       "       [0.5234401 , 0.4765599 ],\n",
       "       [0.2191379 , 0.7808621 ],\n",
       "       [0.77090644, 0.22909356],\n",
       "       [0.69183579, 0.30816421],\n",
       "       [0.83241128, 0.16758872],\n",
       "       [0.23062881, 0.76937119],\n",
       "       [0.77932154, 0.22067846],\n",
       "       [0.21914667, 0.78085333],\n",
       "       [0.77902169, 0.22097831],\n",
       "       [0.77904366, 0.22095634],\n",
       "       [0.1570983 , 0.8429017 ],\n",
       "       [0.71196868, 0.28803132],\n",
       "       [0.20809011, 0.79190989],\n",
       "       [0.56121089, 0.43878911],\n",
       "       [0.60838058, 0.39161942],\n",
       "       [0.68945277, 0.31054723],\n",
       "       [0.71897811, 0.28102189],\n",
       "       [0.45863687, 0.54136313],\n",
       "       [0.43062557, 0.56937443],\n",
       "       [0.45730424, 0.54269576],\n",
       "       [0.4305975 , 0.5694025 ],\n",
       "       [0.21358867, 0.78641133],\n",
       "       [0.4305728 , 0.5694272 ],\n",
       "       [0.77884214, 0.22115786],\n",
       "       [0.18297445, 0.81702555],\n",
       "       [0.77929111, 0.22070889],\n",
       "       [0.77883732, 0.22116268],\n",
       "       [0.79247316, 0.20752684]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_prob = LR.predict_proba(testx)\n",
    "yhat_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(testy, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.Series(pred)\n",
    "sub = pd.DataFrame({'PassengerId': testy1['PassengerId'], 'Survived': new})\n",
    "sub.to_csv(r'C:\\Users\\Sana Shah\\Desktop\\Practice\\Logistic Regression\\Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
